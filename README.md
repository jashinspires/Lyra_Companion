# ‚ú® Lyra ‚Äì Your Empathetic Mental Health Companion

Lyra evokes guidance and light (like the constellation), symbolizing hope, support, and clarity during emotional challenges. It provides empathetic conversation, emotion-aware responses, safe crisis detection, mood tracking, and a virtual persona that mirrors the AI's emotional responses in real time.

Empathetic, AI-powered mental health support‚Äîaccessible 24/7 through a safe, privacy-first conversational experience.

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge)
![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white)
![Pinecone](https://img.shields.io/badge/Pinecone-0A81D1?style=for-the-badge)
![MongoDB](https://img.shields.io/badge/MongoDB-47A248?style=for-the-badge&logo=mongodb&logoColor=white)
![Flutter](https://img.shields.io/badge/Flutter-02569B?style=for-the-badge&logo=flutter&logoColor=white)

## üåü Vision Statement

Normalize seeking mental health support by offering compassionate, culturally aware, always-available help that complements‚Äînot replaces‚Äîlicensed therapy.

## üß© Problem Statement

‚Ä¢ Long wait times and high costs limit access to mental healthcare
‚Ä¢ Stigma prevents early intervention and open conversations
‚Ä¢ Crisis escalation can occur when support is unavailable at critical moments
‚Ä¢ Generic chatbots miss nuance and cultural sensitivity, risking harm
‚Ä¢ Many tools ignore privacy, safety, and ethical guardrails

## üí° Solution Approach

A human-centered, AI-assisted mental health companion:

‚Ä¢ Empathetic conversational agent tuned for supportive, non-judgmental responses
‚Ä¢ Emotion detection from text to tailor coping strategies and resources
‚Ä¢ Safety-first escalation with crisis detection and localized helplines
‚Ä¢ Journaling, grounding exercises, CBT-inspired prompts and mood tracking
‚Ä¢ Multilingual support and culturally sensitive response templates
‚Ä¢ Clear disclaimers and nudges toward professional help when needed

## üß† AI Tools & Architecture

‚Ä¢ LLM Orchestration: LangChain / Instructor for safe prompting and tool use
‚Ä¢ Models: gpt-4o-mini or comparable; distilled local model option for privacy
‚Ä¢ Embeddings/RAG: Pinecone for coping content retrieval
‚Ä¢ Safety Layer: prompt routing, content filters, crisis classifiers
‚Ä¢ Backend: FastAPI with background tasks and rate limiting
‚Ä¢ Frontend: Flutter mobile app and web client (responsive)
‚Ä¢ Data: MongoDB for user profiles, secure journal entries (encrypted at rest)
‚Ä¢ Telemetry: Privacy-preserving analytics (opt-in)

## üîê Ethics, Privacy, and Safety

‚Ä¢ Not a substitute for professional care; disclaimers throughout
‚Ä¢ Crisis protocol: detect self-harm intent, provide region-specific resources, suggest contacting trusted people; never give harmful instructions
‚Ä¢ Data minimization: store only what's necessary, encryption at rest and in transit
‚Ä¢ Transparent logs for moderation; red-team tests for harmful outputs

## üéØ Impact Goals

‚Ä¢ Reduce loneliness and anxiety spikes by offering immediate, empathetic support
‚Ä¢ Encourage help-seeking behavior and early intervention
‚Ä¢ Provide a safe, stigma-free space to reflect and grow

## üìà Success Metrics

‚Ä¢ Session quality (post-chat self-report + sentiment delta)
‚Ä¢ Crisis detection precision/recall and response time
‚Ä¢ Retention of journalers and exercise completion rates
‚Ä¢ Opt-in NPS and qualitative feedback from pilot users

## üôã‚Äç‚ôÇÔ∏è Personal Motivation

Mental health is close to my heart. Friends and peers often struggle silently due to stigma and access barriers. I'm building this to make compassionate support feel reachable‚Äîanytime, anywhere‚Äîwhile honoring safety, ethics, and human dignity.

## üîß Tech Stack

‚Ä¢ Frontend: Flutter (mobile + web), Riverpod/Bloc, Material 3
‚Ä¢ Backend: FastAPI, Uvicorn, MongoDB, Redis (rate limits/queues)
‚Ä¢ AI: OpenAI API or local LLM, LangChain, Pinecone
‚Ä¢ Infra: Docker, GitHub Actions, Fly.io/Render (starter deploy)

## üöÄ Getting Started

```bash
# API
uvicorn app.main:app --reload

# Flutter
flutter run
```

## ü§ù Contributing

We welcome contributions from mental health professionals, designers, and developers. Please open issues for safety concerns or improvement ideas.

## üìû Important Resources

‚Ä¢ International Suicide Hotlines: https://www.opencounseling.com/suicide-hotlines
‚Ä¢ India Helpline (Kiran): 1800-599-0019
‚Ä¢ If you are in immediate danger, contact local emergency services.

Disclaimers: This app does not provide medical advice. It is not a replacement for licensed therapy. If you are in crisis, please seek professional help immediately.
